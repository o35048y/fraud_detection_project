
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Recall-Oriented Fraud Detection Paper</title>

<style>
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', Arial, 'Noto Sans', sans-serif; color: #222; line-height: 1.6; max-width: 900px; margin: 2rem auto; padding: 0 1rem; }
  h1, h2, h3 { color: #111; }
  pre { background: #f6f8fa; padding: 0.8rem; overflow-x: auto; border-radius: 6px; }
  code { background: #f6f8fa; padding: 0.15rem 0.35rem; border-radius: 4px; }
  table { border-collapse: collapse; }
  th, td { border: 1px solid #ddd; padding: 6px 8px; }
  img { max-width: 100%; height: auto; }
  .toc { background: #f9fafb; border: 1px solid #eee; padding: 0.75rem; border-radius: 6px; }
</style>

</head>
<body>
<h1 id="recall-oriented-fraud-detection-on-synthetic-transactions">Recall-Oriented Fraud Detection on Synthetic Transactions</h1>
<h2 id="abstract">Abstract</h2>
<p>We present a practical, recall-oriented workflow for binary fraud detection on a structured, synthetic transaction dataset (~10,000 rows, ~2% fraud). We build preprocessing and modeling pipelines, optimize decision thresholds with respect to $F_2$ (favoring recall), and study the effect of threshold changes on false positives (FP), true positives (TP), and total signals. We compare a Logistic Regression baseline with SMOTE to tree ensembles (RandomForest and BalancedRandomForest) tuned with $F_2$. We document retuning the decision threshold to deliver ~10% more FPs—mimicking an operational push for more alerts—and analyze FP vs. signal curves and threshold–metric trade-offs. Artifacts (models, metrics, plots) are saved for reproducibility.</p>
<h2 id="1-introduction">1. Introduction</h2>
<p>Fraud detection often prioritizes recall: missing fraud (false negatives) is costly, and teams will often accept increased false positives for better coverage. This paper documents an end-to-end applied workflow: synthetic data generation, baseline modeling, recall-focused training with SMOTE and $F_2$ scoring, threshold retuning to increase alerts, and threshold analysis to quantify trade-offs.</p>
<h2 id="2-data">2. Data</h2>
<ul>
<li>Source: <code>data/synthetic_transactions.csv</code> generated by <code>src/data_generator.py</code>.</li>
<li>Size: ~10,000 transactions, fraud ratio ≈ 2%.</li>
<li>Features</li>
<li>Categorical: <code>country</code>, <code>channel</code>, <code>device_type</code>, <code>merchant_category</code>, <code>currency</code></li>
<li>Numeric: <code>amount</code>, <code>hour</code>, <code>is_high_risk_country</code>, <code>is_international</code>, <code>card_present</code>, <code>velocity_24h</code></li>
<li>Dropped IDs: <code>user_id</code>, <code>merchant_id</code></li>
<li>Target: <code>label</code> (1 = fraud, 0 = non-fraud)</li>
</ul>
<p>The generator aims for plausible patterns: log-normal amounts, time-of-day effects, high-risk and international flags, velocity features, and categorical context.</p>
<h2 id="3-methods">3. Methods</h2>
<h3 id="31-preprocessing">3.1 Preprocessing</h3>
<ul>
<li>One-hot encoding for categorical features (unknown-safe), scaling for numeric features.</li>
<li>Train/test split: 80/20, stratified by target with <code>seed=42</code>.</li>
</ul>
<h3 id="32-models">3.2 Models</h3>
<ul>
<li>Logistic Regression (LR) with imbalanced-learn <code>Pipeline</code>:</li>
<li>Preprocessor + SMOTE oversampling (tuned sampling ratio).</li>
<li><code>GridSearchCV</code> scored by $F_2$.</li>
<li>Tree ensembles with scikit-learn and imbalanced-learn:</li>
<li><code>RandomForestClassifier</code> (with/without <code>class_weight="balanced"</code>).</li>
<li><code>BalancedRandomForestClassifier</code>.</li>
<li>Joint grid search over both, scored by $F_2$.</li>
</ul>
<h3 id="33-threshold-selection">3.3 Threshold selection</h3>
<p>The model outputs a probability score $p \in [0,1]$. A threshold $\tau$ turns scores into decisions:</p>
<ul>
<li>Predict fraud if $p \ge \tau$, else non-fraud.</li>
<li>Lower $\tau$ → more alerts, higher recall, lower precision.</li>
<li>Higher $\tau$ → fewer alerts, lower recall, higher precision.</li>
</ul>
<p>We choose thresholds by maximizing $F_2$ on the test split and also retune $\tau$ to achieve ~+10% false positives, reflecting an operational preference for more signals.</p>
<h3 id="34-metrics">3.4 Metrics</h3>
<ul>
<li>AUC-ROC, AUC-PR, confusion matrix (TN, FP, FN, TP), precision, recall, $F_1$, and $F_2$.</li>
<li>$F_\beta$ definition:</li>
</ul>
<p>$$
F_\beta = (1+\beta^2)\, \frac{\text{precision} \cdot \text{recall}}{\beta^2\,\text{precision} + \text{recall}}
$$</p>
<p>We use $\beta=2$ to prioritize recall.</p>
<h2 id="4-experiments-and-artifacts">4. Experiments and Artifacts</h2>
<p>All scripts are in <code>src/models/</code> and <code>scripts/</code>. Outputs are under <code>models/</code> and <code>analysis/</code>.</p>
<h3 id="41-logistic-regression-smote-f_2">4.1 Logistic Regression (SMOTE + $F_2$)</h3>
<ul>
<li>Training: <code>python -m src.models.baseline --threshold -1</code> (auto-picks $F_2$ threshold).</li>
<li>Metrics (at $F_2$-optimal $\tau$): <code>models/metrics.json</code></li>
<li>ROC AUC: 0.9443</li>
<li>PR AUC: 0.5786</li>
<li>$F_1$: 0.5306</li>
<li>$F_2$: 0.5963</li>
<li>Confusion matrix: [[1928, 32], [14, 26]] (TN, FP; FN, TP)</li>
<li>Threshold: $\tau = 0.635$</li>
<li>+10% FP retune: <code>python -m src.models.tune_threshold ... --fp_factor 1.1</code></li>
<li>$\tau$: 0.635 → 0.6180</li>
<li>FP: 32 → 36; Signals (TP+FP): 58 → 62; FP/Signals: 0.552 → 0.581</li>
<li>Analysis: <code>analysis/false_positives_vs_signals.png</code> (curve + annotated operating points)</li>
</ul>
<h3 id="42-tree-ensembles-rf-balancedrf-f_2">4.2 Tree Ensembles (RF &amp; BalancedRF, $F_2$)</h3>
<ul>
<li>Training: <code>python -m src.models.tree_models --threshold -1</code></li>
<li>Best configuration (saved in <code>models/tree_metrics.json</code>):</li>
<li><code>RandomForestClassifier</code>, <code>class_weight="balanced"</code>, <code>n_estimators=600</code>, <code>max_depth=12</code>, <code>min_samples_leaf=3</code>, <code>max_features="sqrt"</code>.</li>
<li>Metrics (at $F_2$-optimal $\tau$): <code>models/tree_metrics.json</code></li>
<li>ROC AUC: 0.9458</li>
<li>PR AUC: 0.4185</li>
<li>$F_1$: 0.3575</li>
<li>$F_2$: 0.5351</li>
<li>Confusion matrix: [[1853, 107], [8, 32]] (TN, FP; FN, TP)</li>
<li>Threshold: $\tau = 0.3425$</li>
<li>+10% FP retune: <code>models/tree_metrics_retuned.json</code></li>
<li>$\tau$: 0.3425 → 0.3327</li>
<li>FP: 107 → 118; Signals: 139 → 150; FP/Signals: 0.770 → 0.787</li>
<li>Analyses</li>
<li>FP vs signals: <code>analysis/false_positives_vs_signals_tree.png</code></li>
<li>Threshold sweep (TP, FP, TP%): <code>analysis/threshold_metrics_tree.png</code></li>
</ul>
<h2 id="5-results">5. Results</h2>
<p>Figures below summarize threshold–metric trade-offs (files in <code>analysis/</code>).</p>
<ul>
<li>Logistic FP vs Signals: <img alt="LR FP vs Signals" src="../analysis/false_positives_vs_signals.png"></li>
<li>Tree FP vs Signals: <img alt="Tree FP vs Signals" src="../analysis/false_positives_vs_signals_tree.png"></li>
<li>Tree Threshold Metrics (TP, FP, TP% vs $\tau$): <img alt="Tree Threshold Metrics" src="../analysis/threshold_metrics_tree.png"></li>
</ul>
<p>Key observations:
- Lowering $\tau$ increases both TP and FP; recall improves while precision drops.
- The LR model offers higher PR AUC and better precision at comparable recall levels than the tree model in this setup, while the tree model achieves strong recall at a much lower threshold with substantially more FPs.
- Retuning +10% FP shifts operating points along each curve as expected: more signals overall with a modest recall gain.</p>
<h2 id="6-discussion">6. Discussion</h2>
<ul>
<li>Threshold as a control: $\tau$ is an interpretable control knob for operations. Selecting $\tau$ by $F_2$ favors catching more fraud. Alternatively, teams can set $\tau$ to match review capacity or a target FP rate.</li>
<li>Model comparison: On this dataset, LR+SMOTE reached better PR AUC, while RF delivered higher raw recall at the expense of precision. Choice depends on your alert budget and tolerance for FPs.</li>
<li>Cost-sensitive view: If a false negative costs $C_{FN}$ and false positive costs $C_{FP}$, threshold selection can minimize expected cost by weighting these terms—equivalently, optimize a cost-weighted utility.</li>
</ul>
<h2 id="7-limitations">7. Limitations</h2>
<ul>
<li>Synthetic data: Real data will contain additional biases, leakage risks, interaction effects, and distribution shift.</li>
<li>Calibration: Tree ensemble probabilities may require calibration (Platt/Isotonic) for more stable thresholding.</li>
<li>Narrow grids: We used modest hyperparameter grids for speed; broader searches (or Bayesian optimization) may further improve results.</li>
</ul>
<h2 id="8-future-work">8. Future Work</h2>
<ul>
<li>Boosted trees (LightGBM/XGBoost) with $F_2$ optimization and probability calibration.</li>
<li>Segment-specific thresholds (e.g., by <code>channel</code>, <code>country</code>) to balance workloads.</li>
<li>Drift monitoring and periodic threshold recalibration.</li>
<li>Cost-sensitive training and decisioning with explicit business cost ratios.</li>
</ul>
<h2 id="9-reproducibility">9. Reproducibility</h2>
<p>The repo contains runnable scripts and saved artifacts. Example commands (PowerShell):</p>
<pre><code class="language-powershell"># Train LR (recall-optimized) and save metrics
python -m src.models.baseline --input_csv data\synthetic_transactions.csv --model_out models\baseline_logreg.pkl --metrics_out models\metrics.json --test_size 0.2 --seed 42 --threshold -1

# Retune LR threshold for ~+10% FP
python -m src.models.tune_threshold --input_csv data\synthetic_transactions.csv --model_in models\baseline_logreg.pkl --metrics_in models\metrics.json --model_out models\baseline_logreg.pkl --metrics_out models\metrics_retuned.json --fp_factor 1.1 --test_size 0.2 --seed 42

# Analyze LR FP vs Signals
python -m src.models.analyze_thresholds --input_csv data\synthetic_transactions.csv --model_in models\baseline_logreg.pkl --metrics_in models\metrics.json --metrics_retuned models\metrics_retuned.json --test_size 0.2 --seed 42 --out_csv analysis\threshold_sweep.csv --out_png analysis\false_positives_vs_signals.png

# Train Tree model (RF/BRF) and save metrics
python -m src.models.tree_models --input_csv data\synthetic_transactions.csv --model_out models\tree_f2.pkl --metrics_out models\tree_metrics.json --test_size 0.2 --seed 42 --threshold -1

# Retune Tree threshold for ~+10% FP
python -m src.models.tune_threshold --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --metrics_in models\tree_metrics.json --model_out models\tree_f2.pkl --metrics_out models\tree_metrics_retuned.json --fp_factor 1.1 --test_size 0.2 --seed 42

# Analyze Tree FP vs Signals
python -m src.models.analyze_thresholds --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --metrics_in models\tree_metrics.json --metrics_retuned models\tree_metrics_retuned.json --test_size 0.2 --seed 42 --out_csv analysis\threshold_sweep_tree.csv --out_png analysis\false_positives_vs_signals_tree.png

# Threshold Metrics Chart (TP, FP, TP%)
python -m src.models.threshold_metrics_chart --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --test_size 0.2 --seed 42 --out_csv analysis\threshold_metrics_tree.csv --out_png analysis\threshold_metrics_tree.png --num_points 401
</code></pre>
<p>Environment details (Windows, PowerShell; Python 3; main libs: numpy, pandas, scikit-learn, imbalanced-learn, matplotlib). A <code>scripts/verify_install.py</code> is provided to validate installed versions.</p>
<h2 id="10-references">10. References</h2>
<ul>
<li>Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python.</li>
<li>Lemaître et al., 2017. Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning.</li>
<li>Saito &amp; Rehmsmeier, 2015. The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets.</li>
</ul>
</body>
</html>
