# Recall-Oriented Fraud Detection on Synthetic Transactions

## Abstract
We present a practical, recall-oriented workflow for binary fraud detection on a structured, synthetic transaction dataset (~10,000 rows, ~2% fraud). We build preprocessing and modeling pipelines, optimize decision thresholds with respect to $F_2$ (favoring recall), and study the effect of threshold changes on false positives (FP), true positives (TP), and total signals. We compare a Logistic Regression baseline with SMOTE to tree ensembles (RandomForest and BalancedRandomForest) tuned with $F_2$. We document retuning the decision threshold to deliver ~10% more FPs—mimicking an operational push for more alerts—and analyze FP vs. signal curves and threshold–metric trade-offs. Artifacts (models, metrics, plots) are saved for reproducibility.

## 1. Introduction
Fraud detection often prioritizes recall: missing fraud (false negatives) is costly, and teams will often accept increased false positives for better coverage. This paper documents an end-to-end applied workflow: synthetic data generation, baseline modeling, recall-focused training with SMOTE and $F_2$ scoring, threshold retuning to increase alerts, and threshold analysis to quantify trade-offs.

## 2. Data
- Source: `data/synthetic_transactions.csv` generated by `src/data_generator.py`.
- Size: ~10,000 transactions, fraud ratio ≈ 2%.
- Features
  - Categorical: `country`, `channel`, `device_type`, `merchant_category`, `currency`
  - Numeric: `amount`, `hour`, `is_high_risk_country`, `is_international`, `card_present`, `velocity_24h`
  - Dropped IDs: `user_id`, `merchant_id`
- Target: `label` (1 = fraud, 0 = non-fraud)

The generator aims for plausible patterns: log-normal amounts, time-of-day effects, high-risk and international flags, velocity features, and categorical context.

## 3. Methods
### 3.1 Preprocessing
- One-hot encoding for categorical features (unknown-safe), scaling for numeric features.
- Train/test split: 80/20, stratified by target with `seed=42`.

### 3.2 Models
- Logistic Regression (LR) with imbalanced-learn `Pipeline`:
  - Preprocessor + SMOTE oversampling (tuned sampling ratio).
  - `GridSearchCV` scored by $F_2$.
- Tree ensembles with scikit-learn and imbalanced-learn:
  - `RandomForestClassifier` (with/without `class_weight="balanced"`).
  - `BalancedRandomForestClassifier`.
  - Joint grid search over both, scored by $F_2$.

### 3.3 Threshold selection
The model outputs a probability score $p \in [0,1]$. A threshold $\tau$ turns scores into decisions:

- Predict fraud if $p \ge \tau$, else non-fraud.
- Lower $\tau$ → more alerts, higher recall, lower precision.
- Higher $\tau$ → fewer alerts, lower recall, higher precision.

We choose thresholds by maximizing $F_2$ on the test split and also retune $\tau$ to achieve ~+10% false positives, reflecting an operational preference for more signals.

### 3.4 Metrics
- AUC-ROC, AUC-PR, confusion matrix (TN, FP, FN, TP), precision, recall, $F_1$, and $F_2$.
- $F_\beta$ definition:

$$
F_\beta = (1+\beta^2)\, \frac{\text{precision} \cdot \text{recall}}{\beta^2\,\text{precision} + \text{recall}}
$$

We use $\beta=2$ to prioritize recall.

## 4. Experiments and Artifacts
All scripts are in `src/models/` and `scripts/`. Outputs are under `models/` and `analysis/`.

### 4.1 Logistic Regression (SMOTE + $F_2$)
- Training: `python -m src.models.baseline --threshold -1` (auto-picks $F_2$ threshold).
- Metrics (at $F_2$-optimal $\tau$): `models/metrics.json`
  - ROC AUC: 0.9443
  - PR AUC: 0.5786
  - $F_1$: 0.5306
  - $F_2$: 0.5963
  - Confusion matrix: [[1928, 32], [14, 26]] (TN, FP; FN, TP)
  - Threshold: $\tau = 0.635$
- +10% FP retune: `python -m src.models.tune_threshold ... --fp_factor 1.1`
  - $\tau$: 0.635 → 0.6180
  - FP: 32 → 36; Signals (TP+FP): 58 → 62; FP/Signals: 0.552 → 0.581
- Analysis: `analysis/false_positives_vs_signals.png` (curve + annotated operating points)

### 4.2 Tree Ensembles (RF & BalancedRF, $F_2$)
- Training: `python -m src.models.tree_models --threshold -1`
- Best configuration (saved in `models/tree_metrics.json`):
  - `RandomForestClassifier`, `class_weight="balanced"`, `n_estimators=600`, `max_depth=12`, `min_samples_leaf=3`, `max_features="sqrt"`.
- Metrics (at $F_2$-optimal $\tau$): `models/tree_metrics.json`
  - ROC AUC: 0.9458
  - PR AUC: 0.4185
  - $F_1$: 0.3575
  - $F_2$: 0.5351
  - Confusion matrix: [[1853, 107], [8, 32]] (TN, FP; FN, TP)
  - Threshold: $\tau = 0.3425$
- +10% FP retune: `models/tree_metrics_retuned.json`
  - $\tau$: 0.3425 → 0.3327
  - FP: 107 → 118; Signals: 139 → 150; FP/Signals: 0.770 → 0.787
- Analyses
  - FP vs signals: `analysis/false_positives_vs_signals_tree.png`
  - Threshold sweep (TP, FP, TP%): `analysis/threshold_metrics_tree.png`

## 5. Results
Figures below summarize threshold–metric trade-offs (files in `analysis/`).

- Logistic FP vs Signals: ![LR FP vs Signals](../analysis/false_positives_vs_signals.png)
- Tree FP vs Signals: ![Tree FP vs Signals](../analysis/false_positives_vs_signals_tree.png)
- Tree Threshold Metrics (TP, FP, TP% vs $\tau$): ![Tree Threshold Metrics](../analysis/threshold_metrics_tree.png)

Key observations:
- Lowering $\tau$ increases both TP and FP; recall improves while precision drops.
- The LR model offers higher PR AUC and better precision at comparable recall levels than the tree model in this setup, while the tree model achieves strong recall at a much lower threshold with substantially more FPs.
- Retuning +10% FP shifts operating points along each curve as expected: more signals overall with a modest recall gain.

## 6. Discussion
- Threshold as a control: $\tau$ is an interpretable control knob for operations. Selecting $\tau$ by $F_2$ favors catching more fraud. Alternatively, teams can set $\tau$ to match review capacity or a target FP rate.
- Model comparison: On this dataset, LR+SMOTE reached better PR AUC, while RF delivered higher raw recall at the expense of precision. Choice depends on your alert budget and tolerance for FPs.
- Cost-sensitive view: If a false negative costs $C_{FN}$ and false positive costs $C_{FP}$, threshold selection can minimize expected cost by weighting these terms—equivalently, optimize a cost-weighted utility.

## 7. Limitations
- Synthetic data: Real data will contain additional biases, leakage risks, interaction effects, and distribution shift.
- Calibration: Tree ensemble probabilities may require calibration (Platt/Isotonic) for more stable thresholding.
- Narrow grids: We used modest hyperparameter grids for speed; broader searches (or Bayesian optimization) may further improve results.

## 8. Future Work
- Boosted trees (LightGBM/XGBoost) with $F_2$ optimization and probability calibration.
- Segment-specific thresholds (e.g., by `channel`, `country`) to balance workloads.
- Drift monitoring and periodic threshold recalibration.
- Cost-sensitive training and decisioning with explicit business cost ratios.

## 9. Reproducibility
The repo contains runnable scripts and saved artifacts. Example commands (PowerShell):

```powershell
# Train LR (recall-optimized) and save metrics
python -m src.models.baseline --input_csv data\synthetic_transactions.csv --model_out models\baseline_logreg.pkl --metrics_out models\metrics.json --test_size 0.2 --seed 42 --threshold -1

# Retune LR threshold for ~+10% FP
python -m src.models.tune_threshold --input_csv data\synthetic_transactions.csv --model_in models\baseline_logreg.pkl --metrics_in models\metrics.json --model_out models\baseline_logreg.pkl --metrics_out models\metrics_retuned.json --fp_factor 1.1 --test_size 0.2 --seed 42

# Analyze LR FP vs Signals
python -m src.models.analyze_thresholds --input_csv data\synthetic_transactions.csv --model_in models\baseline_logreg.pkl --metrics_in models\metrics.json --metrics_retuned models\metrics_retuned.json --test_size 0.2 --seed 42 --out_csv analysis\threshold_sweep.csv --out_png analysis\false_positives_vs_signals.png

# Train Tree model (RF/BRF) and save metrics
python -m src.models.tree_models --input_csv data\synthetic_transactions.csv --model_out models\tree_f2.pkl --metrics_out models\tree_metrics.json --test_size 0.2 --seed 42 --threshold -1

# Retune Tree threshold for ~+10% FP
python -m src.models.tune_threshold --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --metrics_in models\tree_metrics.json --model_out models\tree_f2.pkl --metrics_out models\tree_metrics_retuned.json --fp_factor 1.1 --test_size 0.2 --seed 42

# Analyze Tree FP vs Signals
python -m src.models.analyze_thresholds --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --metrics_in models\tree_metrics.json --metrics_retuned models\tree_metrics_retuned.json --test_size 0.2 --seed 42 --out_csv analysis\threshold_sweep_tree.csv --out_png analysis\false_positives_vs_signals_tree.png

# Threshold Metrics Chart (TP, FP, TP%)
python -m src.models.threshold_metrics_chart --input_csv data\synthetic_transactions.csv --model_in models\tree_f2.pkl --test_size 0.2 --seed 42 --out_csv analysis\threshold_metrics_tree.csv --out_png analysis\threshold_metrics_tree.png --num_points 401
```

Environment details (Windows, PowerShell; Python 3; main libs: numpy, pandas, scikit-learn, imbalanced-learn, matplotlib). A `scripts/verify_install.py` is provided to validate installed versions.

## 10. References
- Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python.
- Lemaître et al., 2017. Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning.
- Saito & Rehmsmeier, 2015. The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets.
