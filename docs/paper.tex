\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{float}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}

\graphicspath{{../analysis/}}

\title{Recall-Oriented Fraud Detection on Synthetic Transactions}
\author{Nello Di Candia}
\date{November 3, 2025}

\begin{document}
\maketitle

\begin{abstract}
We present a practical, recall-oriented workflow for binary fraud detection on a structured, synthetic transaction dataset ($\sim$ \num{10000} rows, $\sim$2\% fraud). We build preprocessing and modeling pipelines, optimize decision thresholds with respect to $F_2$ (favoring recall), and study the effect of threshold changes on false positives (FP), true positives (TP), and total signals. We compare a Logistic Regression baseline with SMOTE to tree ensembles (RandomForest and BalancedRandomForest) tuned with $F_2$. We document retuning the decision threshold to deliver $\sim$10\% more FPs---mimicking an operational push for more alerts---and analyze FP vs. signal curves and threshold--metric trade-offs. Artifacts (models, metrics, plots) are saved for reproducibility.
\end{abstract}

\section{Introduction}
Fraud detection often prioritizes recall: missing fraud (false negatives, FN) is costly, and teams will often accept increased false positives (FP) for better coverage. This paper documents an end-to-end applied workflow: synthetic data generation, baseline modeling, recall-focused training with SMOTE and $F_2$ scoring, threshold retuning to increase alerts, and threshold analysis to quantify trade-offs.

\section{Data}
\textbf{Source}: \texttt{data/synthetic\_transactions.csv} generated by \texttt{src/data\_generator.py}. Size: $\sim$10,000 transactions, fraud ratio $\approx$ 2\%.

\textbf{Features}: Categorical: \texttt{country}, \texttt{channel}, \texttt{device\_type}, \texttt{merchant\_category}, \texttt{currency}. Numeric: \texttt{amount}, \texttt{hour}, \texttt{is\_high\_risk\_country}, \texttt{is\_international}, \texttt{card\_present}, \texttt{velocity\_24h}. Dropped IDs: \texttt{user\_id}, \texttt{merchant\_id}. Target: \texttt{label} (1 = fraud, 0 = non-fraud).

The generator introduces plausible patterns: log-normal amounts, time-of-day effects, high-risk and international flags, velocity features, and categorical context.

\section{Methods}
\subsection{Preprocessing}
One-hot encoding for categorical features (unknown-safe) and scaling for numeric features. Train/test split: 80/20, stratified by target with \texttt{seed = 42}.

\subsection{Models}
\textbf{Logistic Regression (LR)} with imbalanced-learn \texttt{Pipeline}: Preprocessor + SMOTE oversampling (tuned sampling ratio), \texttt{GridSearchCV} scored by $F_2$.

\textbf{Tree ensembles}: \texttt{RandomForestClassifier} (with/without \texttt{class\_weight=balanced}) and \break \texttt{BalancedRandomForestClassifier} with joint grid search scored by $F_2$.

\subsection{Threshold selection}
The model outputs a probability score $p \in [0,1]$. A threshold $\tau$ converts scores into decisions: predict fraud if $p \geq \tau$, else non-fraud. Lower $\tau$ increases alerts (higher recall, lower precision); higher $\tau$ reduces alerts (lower recall, higher precision). We choose thresholds by maximizing $F_2$ on the test split and also retune $\tau$ to achieve $\sim$10\% more false positives.

\subsection{Metrics}
We report AUC-ROC, AUC-PR, confusion matrix (TN, FP; FN, TP), precision, recall, $F_1$, and $F_2$. The $F_\beta$ measure is defined as
\begin{equation}
F_\beta = (1+\beta^2)\, \frac{\text{precision} \cdot \text{recall}}{\beta^2\,\text{precision} + \text{recall}},
\end{equation}
with $\beta=2$ to prioritize recall.

\section{Experiments and Artifacts}
Scripts reside in \texttt{src/models/} and \texttt{scripts/}. Outputs are under \texttt{models/} and \texttt{analysis/}.

\subsection{Logistic Regression (SMOTE + $F_2$)}
Metrics at the $F_2$-optimal threshold ($\tau=0.635$), saved in \texttt{models/metrics.json}: ROC AUC 0.9443; PR AUC 0.5786; $F_1$ 0.5306; $F_2$ 0.5963; confusion matrix $\big[\big[1928, 32\big], \big[14, 26\big]\big]$ (TN, FP; FN, TP). Retuning to \texttt{+10\% FP}: $\tau$ 0.635 $\to$ 0.6180; FP 32 $\to$ 36; signals 58 $\to$ 62; FP/signals 0.552 $\to$ 0.581.

\subsection{Tree Ensembles (RF \& BalancedRF, $F_2$)}
Best configuration (\texttt{models/tree\_metrics.json}): \texttt{RandomForestClassifier}, \texttt{class\_weight=balanced}, \break $n_{\text{estimators}}=600$, $\text{max\_depth}=12$, $\text{min\_samples\_leaf}=3$, $\text{max\_features}=\text{sqrt}$. Metrics at $F_2$-optimal $\tau$ (0.3425): ROC AUC 0.9458; PR AUC 0.4185; $F_1$ 0.3575; $F_2$ 0.5351; confusion matrix $\big[\big[1853, 107\big], \big[8, 32\big]\big]$. Retuning \texttt{+10\% FP}: $\tau$ 0.3425 $\to$ 0.3327; FP 107 $\to$ 118; signals 139 $\to$ 150; FP/signals 0.770 $\to$ 0.787.

\section{Results}
\begin{table}[H]
  \centering
  \caption{Methods comparison: performance and efficiency}
  \begin{tabular}{lrrrr}
    \toprule
    Method & F2 & PR~AUC & Model Size (MB) & Throughput (ex/s) \\ 
    \midrule
    LogReg+SMOTE (F2) & 0.5963 & 0.5786 & 0.0596 & 501,822 \\
    RandomForest (F2) & 0.5351 & 0.4185 & 10.9378 & 19,316 \\
    \bottomrule
  \end{tabular}
  \label{tab:methods-comparison}
\end{table}

Lowering $\tau$ increases both TP and FP; recall improves, precision drops. The LR model offers higher PR AUC and better precision at comparable recall levels than the tree model in this setup, while the tree model achieves strong recall at a much lower threshold with substantially more FPs. Retuning \texttt{+10\% FP} shifts operating points along each curve as expected: more signals overall with modest recall gains.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{false_positives_vs_signals.png}
  \caption{Logistic Regression: FP vs. total signals across thresholds; annotated baseline/retuned points.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{false_positives_vs_signals_tree.png}
  \caption{Tree Ensembles: FP vs. total signals across thresholds; annotated baseline/retuned points.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{threshold_metrics_tree.png}
  \caption{Tree Ensembles: Threshold sweep showing total signals (TP+FP), FP, and TP\% detected (recall\%).}
\end{figure}

\section{Discussion}
\textbf{Threshold as a control}: $\tau$ is an interpretable knob for operations. Selecting $\tau$ by $F_2$ favors catching more fraud; alternatively, set $\tau$ to match review capacity or a target FP rate.

\textbf{Model comparison}: On this dataset, LR+SMOTE reached better PR AUC, while RF delivered higher raw recall at the expense of precision. Choice depends on alert budget and tolerance for FPs.

\textbf{Cost-sensitive view}: If a false negative costs $C_{FN}$ and a false positive costs $C_{FP}$, threshold selection can minimize expected cost by weighting these terms, i.e., optimize a cost-weighted utility.

\section{Limitations}
Synthetic data may not capture real-world biases, leakage, interaction effects, or distribution shift. Tree ensemble probabilities may require calibration (Platt/Isotonic) for stable thresholding. Hyperparameter grids were modest for speed; broader searches or Bayesian optimization may further improve results.

\section{Future Work}
Boosted trees (LightGBM/XGBoost) with $F_2$ optimization and probability calibration; segment-specific thresholds (e.g., by \texttt{channel}, \texttt{country}); drift monitoring and periodic threshold recalibration; cost-sensitive training/decisioning with explicit business cost ratios.

\section{Reproducibility}
Artifacts and scripts are provided in the repository. Key outputs reside in \texttt{models/} and \texttt{analysis/}. Environment: Windows, PowerShell; Python 3; core libs: NumPy, pandas, scikit-learn, imbalanced-learn, matplotlib. A script \texttt{scripts/verify\_install.py} validates installed versions.

\end{document}
